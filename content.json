{"posts":[{"title":"AI generated images literature review","text":"Notion Blog Post Link &lt;- Click Here! Checkout the Notion Blog in the Title!","link":"/blog/2023/05/06/aigc-literature-review/"},{"title":"Build a deep-learning workstation","text":"Build a deep-learning workstation Motivation: not rely on any lab’s computing resource, learn &amp; experiment with public deep learning repos. I can’t replicate the results on my XPS 15 RTX 3050TI 4GB GPU, which forces me to look at other resources. Looking at the pricing and setup needed to work with cloud computing, I found better returns by building my own deep-learning workstation. Trying and replicating the results of these repos are absolutely important for learning and creativity. Crazy GPU Speed isn’t my focus at the moment, but large GPU mem is the key. 3060 seems to be the most price efficient one in the market at this point. Videos referenced: Which GPU to choose for deep learning? How to Choose an NVIDIA GPU for Deep Learning in 2023: Ada, Ampere, GeForce, NVIDIA RTX Compared How to Build a Deep Learning Machine - Everything You Need To Know How to Assemble a Deep Learning Machine - Full Process | Part 2 How To Install CUDA, cuDNN, Ubuntu, Miniconda | ML Software Stack | Part 3/3 (Optional) Article references which-gpu-for-deep-learning deep-learning-hardware-guide Specs of my machine (for replication purpose): Price: $924.99 CPU: AMD Ryzen 7-5800 Memory: 16GB GPU: NVIDIA GeForce RTX 3060 12GB SSD: 1TB (Samsung 970 evo plus) +$69.99 🔗 Follow this video for in-depth environment installation instruction How To Install CUDA, cuDNN, Ubuntu, Miniconda | ML Software Stack | Part 3/3 📘 Official documentation referenced from How To Install CUDA, cuDNN, Ubuntu, Miniconda | ML Software Stack | Part 3/3 ✅ Ubuntu installation ✅ CUDA installation ✅ cuDNN installation SSH-ing Step 1: Step up ssh from remote laptops (e.g. mac/windows) to Ubuntu within LAN How to enable SSH on Linux Ubuntu (Easy step by step guide) Step 2: Step up ssh port forwarding to connect from any network through remote laptops (e.g. mac/windows) to Ubuntu windows 系统如何通过 ssh 远程连接 linux？ - 知乎 cpolar - 安全的内网穿透工具 I used the free account that fulfills my personal usage and need. Works perfectly at the moment. There can be other cleaner approaches, but I haven’t find such one. Please send me a message if you found one. Step 3: passwordless login instructions How to connect without password using SSH (passwordless) Thanks for reading, if you have any questions/suggestions, please leave a comment below.","link":"/blog/2023/04/23/deep-learning-workstation/"},{"title":"科目一 ｜ 上海","text":"94/100 一遍 -&gt; PASS 科目一 Resources Sources helped me prep 科目一 in Shanghai, China (C2 licence in China). 大部分除了标志的考点这边总结的很全 https://zhuanlan.zhihu.com/p/556065386 交通标志的 source（全的 https://www.jiakaobaodian.com/sign/2/ Preparation Process (about 3~4 whole days) 我精简 500 道（一开始刷要多理解理解，会比较慢）完，然后模考了 10 次（三天每天 3 套，然后总结总结，后面很多题目重复的多了就变快很多）。上海题和新规题多看（我把上海题刷完了，没多少道，比较 tricky）","link":"/blog/2023/06/01/kemu1/"},{"title":"CS 189 Intro to Learning Theory","text":"CS 189 Learning Theory blog This is a blog post for CS 189 lecture on Learning Theory, contents are organized in the flow of the lecture. This blog isn’t the final version yet. A range space is a pair $P$, $H$: $P$ is set of all possible test/training points (can be infinite) $H$ is a hypothesis class (set the boundary of legal classifiers), a set of hypotheses (classifiers) e.g. all the linear classifiers Suppose all training pts &amp; test pts are drawn independently from same prob. distribution $\\mathcal D$ defined on domain $P$ $h\\in \\mathcal H$ be a hypothesis (a classifier), h predicts a pt x is in class C if $x\\in h$ Risk (generalization error) $R(h)$ of $h$ is the probability that $h$ misclassifies a random pt $x$ drawn from $\\mathcal D$ (i.e. the prob that $x\\in C$ but $\\notin h$) ← essentially test error risk is the average test error for test points drawn randomly from $\\mathcal D$ (但我们一般是一个 subset of the theoretical entire set of $\\mathcal D$, so test error sometimes is high, sometimes is low). Notation: Let $X\\subseteq P$ be a set of $n$ training pts drawn from $\\mathcal D$ Empirical risk (training error) $\\hat R(h)$ is % of $X$ misclassified by $h$ $h$ misclassfies each training pt w/ prob. $R(h)$, so total misclassified has a binomial distributio. $P(|\\hat R(h)-R(h)|&gt;\\epsilon) \\le2e^{-2\\epsilon^2n}$ when n is very large, the probability gets smaller. Idea for learning algorithm: choose $\\hat h\\in H$ that minimizes $\\hat R(\\hat h)$ ← empirical risk minimization Problem: if too many hypotheses, some $h$ with high $R(h)$ will get lucky and have very low $\\hat R(h)$ we can have so many hypotheses that some of them just get lucky and score far lower training error than their actual risk. → This is overfitting Dichotomies a dichotomy of $X$ is $X \\cap h$, where $h\\in H$ picks out the training points that $h$ predicts are in class $C$ e.g. for $n$ training points, there could be up to $2^n$ dichotomies. $\\hat R(\\hat h) = 0$ even if every $h\\in H$ has high risk we simply overfits We limit to have a constant $\\prod$ dichotomies, $P(\\text{at least one dichotomy has } |\\hat R - R|\\ge\\epsilon)\\le\\delta$ $\\delta = 2\\prod e^{-2\\epsilon^2n}$ Hence with prob. $\\ge 1-\\delta$ for every $h\\in H$ $\\downarrow$ complement of the above inequality $|\\hat R(h) - R(h)| \\le \\epsilon = \\sqrt{\\frac 1{2n}ln\\frac{2\\prod}{\\delta}}$ 💡Lesson: the smaller we make $\\prod$, the number of dichotomies, and larger the n, the number of training points, the more accurately the training error will approximate how well the classifier performs on test data. Smaller $\\prod$ means we’re less likely to overfit, we have less variance, but more bias. But it doesn’t mean that risk is small. Goal: we want a H that fits the data well and doesn’t produce many dichotomies. Let $h^* \\in H$ minimizes $R(h^*)$: “best” classifier Let $\\hat h\\in H$ minimizes $\\hat R(\\hat h)$; the classifier we learn, with prob $\\ge 1-\\delta$, our chosen $\\hat h$ has nearly optimal risk: Note: we still choose the best classifier $\\hat h$ that minimizes the empirical risk. We don’t know the actual $h^*$ since we have no means to know it. But if $\\prod$ is small and $n$ is large, the hypothesis $\\hat h$ we have chosen is probably nearly as good as $h^{*}$. We have the following inequalities: $R(\\hat h) \\le \\hat R(\\hat h) +\\epsilon$ got from previous inequalities since $\\hat R$ is lowest on classifiers trained on training data, it’s not the lowest on the theoretical optimal $\\hat h$. Then, using the previous inequality again, we have $\\hat R (h^{*})+ \\epsilon \\le R(h^*) + 2\\epsilon$ To sum up, Sample complexity: the # of training pts needed to achieve this $\\epsilon$ with prob $\\ge 1-\\epsilon$","link":"/blog/2023/05/07/learning-theory/"},{"title":"Latex aligning equations","text":"latex equation align Daily tip of new latex skills. I was search for ways to align multiple parts of an equation, and found this post on stackexchange. The align environment is the vanilla way to align equations, and the &amp; symbol is used to specify the point of alignment. 1234\\begin{align} a &amp;= b + c \\\\ &amp;= d + e\\end{align} $$ \\begin{align} a &amp;= b + c\\\\ &amp;= d + e \\end{align} $$ To specify more than one point of alignment, use the alignat environment. The first argument is the number of points of alignment, and the second argument is the number of columns in the equation. 123456\\begin{alignat*}{3}&amp; m \\quad &amp;&amp; \\text{módulo} \\quad &amp;&amp; m&gt;0\\\\&amp; a \\quad &amp;&amp; \\text{multiplicador} \\quad &amp;&amp; 0&lt;a&lt;m\\\\&amp; c \\quad &amp;&amp; \\text{constante aditiva} \\quad &amp;&amp; 0\\leq c&lt;m\\\\&amp; x_0 \\quad &amp;&amp; \\text{valor inicial} \\quad &amp;&amp; 0\\leq x_0 &lt;m\\end{alignat*} Additionally, there’s a discussion of what’s the difference between &amp; and &amp;&amp; in this thread, feel free to checkout this for more information! What does a double ampersand (&amp;&amp;) mean in LaTeX? Thanks for reading, if you have any questions/suggestions, please leave a comment below.","link":"/blog/2023/04/30/latex-equation-align/"},{"title":"template post with syntax","text":"Source This is a page that showcases all the syntax that are supported by fluid and hexo. Index image in home page 12345678---title: 文章标题tags: [Hexo, Fluid]index_img: /img/example.jpgdate: 2019-10-10 10:00:00---以下是文章内容 Codeblocks 123import numpy as npimport pandas as pdimport matplotlib.pyplot as plt Latex formulas $$ E=mc^2 $$ Mermaid diagrams [^1] [^1]: Links: Source; Github Gannt diagram classDiagram classDiagram Class01 C2 : Where am i? Class09 --* C3 Class09 --|> Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 C2: Cool label [^2]: This is the footnote content [^3]: This is the footnote content Graph graph TD; A-->B; A-->C; B-->D; C-->D; Sequence diagram sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! Git graph gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit Flowchart flowchart TD A[Start] --> B{Is it?} B -- Yes --> C[OK] C --> D[Rethink] D --> B B -- No ----> E[End] Using footnotes This is a sentence[^2] This is a sentence[^2][^3] This is a sentence[^2] This is a sentence[^2]","link":"/blog/2023/04/21/template-post-with-syntax/"}],"tags":[{"name":"aigc","slug":"aigc","link":"/blog/tags/aigc/"},{"name":"driving","slug":"driving","link":"/blog/tags/driving/"},{"name":"ml","slug":"ml","link":"/blog/tags/ml/"},{"name":"latex","slug":"latex","link":"/blog/tags/latex/"}],"categories":[],"pages":[{"title":"about","text":"Transferring back to main page after clicking the button… Home","link":"/blog/about/index.html"}]}