<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Kernalization</title>
    <link href="/blog/2023/05/07/Kernalization/"/>
    <url>/blog/2023/05/07/Kernalization/</url>
    
    <content type="html"><![CDATA[<h1>CS 189 Lecture 16: Kernelization</h1><aside>💡 **Motivation**: with d input features, degree-p polynomials blow up to $O(d^p)$ features.<ul><li>weights can be written as a linear combo of sample pts</li><li>we can use the inner product of $\Phi(x)$ only</li></ul></aside><h1>1. Kernal Ridge</h1><p>Step 1: preprocess</p><p><img src="https://s2.loli.net/2023/05/08/B4sFfat2QlIgVMz.png" alt="Untitled.png"></p><p><img src="https://s2.loli.net/2023/05/08/6rfMFUOkazKV142.png" alt="Untitled 1.png"></p><p>We don’t have $(X^T X+\lambda I’)w$ because of a justification, we need w be a linear combination of sample points.</p><p><img src="https://s2.loli.net/2023/05/08/EZPRqB83f42XtVG.png" alt="Untitled 2.png"></p><p><img src="https://s2.loli.net/2023/05/08/2pfH5bglDLPBhYQ.png" alt="Untitled 3.png"></p><h2 id="1-1-Kernel">1.1 Kernel</h2><blockquote><p>和之前完全一样的 formulation，只是我们把换了一个 notation，introduce 了 kernel，之后我们会换别的 kernel function。</p></blockquote><p>$k(x,z)=x^Tz$ be a kernel fn.</p><p>Let $K = XX^T$ be a nxn kernel matrix, $K$ is the kernel matrix</p><p>$K_{ij} = k(X_i, X_j)$</p><p><img src="https://s2.loli.net/2023/05/08/INUSFaqKTvZrcm6.png" alt="Untitled 4.png"></p><p><img src="https://s2.loli.net/2023/05/08/xUd7Pq8SFu1W4Hi.png" alt="Untitled 5.png"></p><ul><li>comparison of runtime between dual and primal problems.</li></ul><h2 id="1-2-The-Kernel-Trick-kernelization">1.2 The Kernel Trick (kernelization)</h2><p><img src="https://s2.loli.net/2023/05/08/ECvBDhFUb6ey2ft.png" alt="Untitled 6.png"></p><p>The factor of $\sqrt 2$ doesn’t matter that much in practice.</p><p>$\Phi(x)$, $\Phi (z)$ are vectors, they are defined symbolically.</p><p>!! don’t compute $\Phi(x)$ or $\Phi(z)$, compute $k(x, z)=(x^Tz+1)^p$</p><p><img src="https://s2.loli.net/2023/05/08/9nC2oNadf1BblJw.png" alt="Untitled 7.png"></p><ul><li>The running time is sublinear, actually much smaller than linear, in the length of the $\Phi$ vectors.</li></ul><h1>2. Kernel Perceptrons</h1><h2 id="2-1-Original-Perceptron-formulation">2.1. Original Perceptron formulation:</h2><p><img src="https://s2.loli.net/2023/05/08/oP1LcDukA5JdYg8.png" alt="Untitled 8.png"></p><p><img src="https://s2.loli.net/2023/05/08/qAVdG42CUphO1zi.png" alt="Untitled 9.png"></p><ol><li>Initialize $w$</li><li>Uses stochastic gradient descent<ol><li>use gradients of the misclassified points to do gradient descents</li><li>gradient is $-y_i\Phi(X_i)$</li><li>so, we do this update $w \leftarrow w+\epsilon y_i\Phi(X_i)$ since $-(-…)=+…$</li></ol></li><li>On testing, since perceptron is doing $sign(X_i\cdot w)= class$</li></ol><h2 id="2-2-Dualized-version">2.2. Dualized version</h2><p>Let $\Phi(X)$ be $n\times D$ matrix with rows $\Phi(X_i)^T$, $D=$ length of $\Phi(.)$, $K=\Phi(X)\Phi(X)^T$</p><p><img src="https://s2.loli.net/2023/05/08/3smMYhE6nG4y5JF.png" alt="Untitled 10.png"></p><p><img src="https://s2.loli.net/2023/05/08/Zchr3goaLBRQs2O.png" alt="Untitled 11.png"></p><p><img src="https://s2.loli.net/2023/05/08/9lWrIO7FMu6zdn2.png" alt="Untitled 12.png"></p><h1>3. Kernel Logistic Regression</h1><p><img src="https://s2.loli.net/2023/05/08/qdZDk23jS7zEQNp.png" alt="Untitled 13.png"></p><h1>4. Gaussian Kernel (let D be infinite dimension)</h1><p><img src="https://s2.loli.net/2023/05/08/6CpQ4D1gtJ7Eima.png" alt="Untitled 14.png"></p><p>We can inspect what the $\Phi(x)$ is, but we don’t use it in practice</p><p><img src="https://s2.loli.net/2023/05/08/b1MJdZ3VFrIP6YK.png" alt="Untitled 15.png"></p><aside>💡 It’s best ******not****** to think of points in a high-d space. It’s no longer a useful intuition. Instead, think of the kernel $k$ as a measure of how similar or close together two points</aside><p>Referencing back to the <strong>dual ridge regression</strong>, now the $h(z)$ in test time is a linear combination of Gaussians centered at x. $h(z)=\sum_{j=1}^na_j\cdot k(X_j,z)$ ← dual weights are the coefficients</p><p><img src="https://s2.loli.net/2023/05/08/fkIWSrO4y26GnYA.png" alt="Untitled 16.png"></p><ul><li>Intuition from Gaussian kernel function (a measure of similarity)<br><img src="https://s2.loli.net/2023/05/08/vg6LBriAox9wd7F.png" alt="Untitled 17.png"></li></ul><p>The Gaussian kernel is very popular in practice</p><p><img src="https://s2.loli.net/2023/05/08/5qlmchoiZjbAIDN.png" alt="Untitled 18.png"></p><aside>💡 Note: we treat $\sigma$ in kernel function as a hyper-parameter. Larger $\sigma$ → wider Gaussians & smoother $h$ → more bias & less variance.</aside><p>A function is qualified to be a kernel function only if it always generates a PSD kernel matrix for every sample.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Intro to Learning Theory</title>
    <link href="/blog/2023/05/07/learning-theory/"/>
    <url>/blog/2023/05/07/learning-theory/</url>
    
    <content type="html"><![CDATA[<h1>CS 189 Learning Theory blog</h1><div class="note note-primary">            <p>This is a blog post for CS 189 lecture on Learning Theory, contents are organized in the flow of the lecture. This blog isn’t the final version yet.</p>          </div><p>A range space is a pair $P$, $H$:</p><p>$P$ is <strong>set of all possible test/training points</strong> (can be infinite)</p><p>$H$ is a <strong>hypothesis class</strong> (set the boundary of legal classifiers), a set of hypotheses (classifiers)</p><ul><li>e.g. all the linear classifiers</li></ul><p>Suppose all training pts &amp; test pts are drawn <em>independently</em> from same prob. distribution $\mathcal D$ defined on domain $P$</p><p>$h\in \mathcal H$ be a hypothesis (a classifier), h predicts a pt x is in class C if $x\in h$</p><p>Risk (generalization error) $R(h)$ of $h$ is the probability that $h$ misclassifies a random pt $x$ drawn from $\mathcal D$ (i.e. the prob that $x\in C$ but $\notin h$) ← essentially test error</p><ul><li>risk is the average test error for test points drawn randomly from $\mathcal D$ (但我们一般是一个 subset of the theoretical entire set of $\mathcal D$, so test error sometimes is high, sometimes is low).</li></ul><p>Notation:</p><p>Let $X\subseteq P$ be a set of $n$ training pts drawn from $\mathcal D$</p><p>Empirical risk (training error) $\hat R(h)$ is % of $X$ misclassified by $h$</p><p>$h$ misclassfies each training pt w/ prob. $R(h)$, so total misclassified has a binomial distributio.</p><p>$P(|\hat R(h)-R(h)|&gt;\epsilon) \le2e^{-2\epsilon^2n}$</p><ul><li>when n is very large, the probability gets smaller.</li></ul><p>Idea for learning algorithm: choose $\hat h\in H$ that minimizes $\hat R(\hat h)$ ← empirical risk minimization</p><p>Problem: if too many hypotheses, some $h$ with high $R(h)$ will get lucky and have very low $\hat R(h)$</p><ul><li>we can have so many hypotheses that some of them just get lucky and score far lower training error than their actual risk.<br>→ This is overfitting</li></ul><p>Dichotomies</p><ul><li>a dichotomy of $X$ is $X \cap h$, where $h\in H$<ul><li>picks out the training points that $h$ predicts are in class $C$</li><li>e.g. for $n$ training points, there could be up to $2^n$ dichotomies.</li><li>$\hat R(\hat h) = 0$ even if every $h\in H$ has high risk</li><li>we simply overfits</li></ul></li></ul><p>We limit to have a constant $\prod$ dichotomies,</p><p>$P(\text{at least one dichotomy has } |\hat R - R|\ge\epsilon)\le\delta$</p><ul><li>$\delta = 2\prod e^{-2\epsilon^2n}$</li><li>Hence with prob. $\ge 1-\delta$ for every $h\in H$</li><li>$\downarrow$ complement of the above inequality</li><li>$|\hat R(h) - R(h)| \le \epsilon = \sqrt{\frac 1{2n}ln\frac{2\prod}{\delta}}$</li></ul><div class="note note-info">            <p>💡Lesson: the smaller we make $\prod$, the number of dichotomies, and larger the n, the number of training points, the more accurately the training error will approximate how well the classifier performs on test data.</p>          </div><p>Smaller $\prod$ means we’re less likely to overfit, we have less variance, but more bias. But it doesn’t mean that risk is small.</p><ul><li>Goal: we want a H that fits the data well and doesn’t produce many dichotomies.</li></ul><p>Let $h^* \in H$ minimizes $R(h^*)$: “best” classifier</p><p>Let $\hat h\in H$ minimizes $\hat R(\hat h)$; the classifier we learn, with prob $\ge 1-\delta$, our chosen $\hat h$ has nearly optimal risk:</p><blockquote><p>Note: we still choose the best classifier $\hat h$ that minimizes the empirical risk. We don’t know the actual $h^*$ since we have no means to know it. But if $\prod$ is small and $n$ is large, the hypothesis $\hat h$ we have chosen is probably nearly as good as $h^{*}$.</p></blockquote><p>We have the following inequalities:</p><p>$R(\hat h) \le \hat R(\hat h) +\epsilon$</p><ul><li>got from previous inequalities</li></ul><p>since $\hat R$ is lowest on classifiers trained on training data, it’s not the lowest on the theoretical optimal $\hat h$.</p><p>Then, using the previous inequality again, we have<br>$\hat R (h^{*})+ \epsilon \le R(h^*) + 2\epsilon$</p><p>To sum up,</p><p><img src="https://s2.loli.net/2023/05/08/gYk8ltA9ZPvf7nM.png" alt=""></p><p>Sample complexity: the # of training pts needed to achieve this $\epsilon$ with prob $\ge 1-\epsilon$</p>]]></content>
    
    
    
    <tags>
      
      <tag>ml</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI generated images literature review</title>
    <link href="/blog/2023/05/06/aigc-literature-review/"/>
    <url>/blog/2023/05/06/aigc-literature-review/</url>
    
    <content type="html"><![CDATA[<h1><a href="https://bold-tortellini-108.notion.site/Literature-Review-AI-61a89e4b56fc4a4e899aa387c6b05c91">Notion Blog Post Link &lt;- Click Here!</a></h1><div class="note note-warning">            <p>Checkout the Notion Blog in the Title!</p>          </div><p><img src="https://cdn.arstechnica.net/wp-content/uploads/2022/09/ai_art_on_shutterstock_hero.jpg" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>aigc</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Build a deep-learning workstation</title>
    <link href="/blog/2023/04/23/deep-learning-workstation/"/>
    <url>/blog/2023/04/23/deep-learning-workstation/</url>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p><strong>Motivation</strong>:<br>not rely on any lab’s computing resource, <strong>learn &amp; experiment with</strong> public deep learning repos. I can’t replicate the results on my XPS 15 RTX 3050TI 4GB GPU, which forces me to look at other resources. Looking at the pricing and setup needed to work with cloud computing, I found better returns by building my own deep-learning workstation. Trying and replicating the results of these repos are absolutely important for learning and creativity. Crazy GPU Speed isn’t my focus at the moment, but large GPU mem is the key. 3060 seems to be the most price efficient one in the market at this point.</p>          </div><p><img src="https://s2.loli.net/2023/05/08/kfaZXiT4WUlRFr7.jpg" alt=""><br><img src="https://s2.loli.net/2023/05/08/NQOLasbGrAmSy5u.jpg" alt=""></p><h1>Videos referenced:</h1><p>Which GPU to choose for deep learning?</p><p><a href="https://www.youtube.com/watch?v=F1ythHjdWI0">How to Choose an NVIDIA GPU for Deep Learning in 2023: Ada, Ampere, GeForce, NVIDIA RTX Compared</a></p><p><a href="https://www.youtube.com/watch?v=OWvy-fCWTBQ">How to Build a Deep Learning Machine - Everything You Need To Know</a></p><p><a href="https://www.youtube.com/watch?v=XIHG11EzB28">How to Assemble a Deep Learning Machine - Full Process | Part 2</a></p><p><a href="https://www.youtube.com/watch?v=ttxtV966jyQ">How To Install CUDA, cuDNN, Ubuntu, Miniconda | ML Software Stack | Part 3/3</a></p><h2 id="Optional-Article-references">(Optional) Article references</h2><p><a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#">which-gpu-for-deep-learning</a></p><p><a href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">deep-learning-hardware-guide</a></p><h1>Specs of my machine (for replication purpose):</h1><div class="note note-info">            <p><strong>Price</strong>: $924.99<br><strong>CPU</strong>: AMD Ryzen 7-5800<br><strong>Memory:</strong> 16GB<br><strong>GPU:</strong> NVIDIA GeForce RTX 3060 12GB<br><strong>SSD:</strong> 1TB (Samsung 970 evo plus) +$69.99</p>          </div><p><a href="https://www.bestbuy.com/site/lenovo-legion-tower-5-amd-gaming-desktop-amd-ryzen-7-5800-16gb-memory-nvidia-geforce-rtx-3060-256gb-ssd-1tb-hdd-raven-black/6501812.p?skuId=6501812"></a></p><div class="note note-info">            <p>🔗 Follow this video for in-depth environment installation instruction <a href="https://www.youtube.com/watch?v=ttxtV966jyQ">How To Install CUDA, cuDNN, Ubuntu, Miniconda | ML Software Stack | Part 3/3</a></p>          </div><div class="note note-info">            <p>📘 Official documentation referenced from <code>How To Install CUDA, cuDNN, Ubuntu, Miniconda | ML Software Stack | Part 3/3</code></p>          </div><ol><li><p>✅ <a href="https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview">Ubuntu installation</a></p></li><li><p>✅ <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">CUDA installation</a></p></li><li><p>✅ <a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html">cuDNN installation</a></p></li></ol><h1>SSH-ing</h1><h2 id="Step-1-Step-up-ssh-from-remote-laptops-e-g-mac-windows-to-Ubuntu-within-LAN">Step 1: Step up ssh from remote laptops (e.g. mac/windows) to Ubuntu <em>within LAN</em></h2><p><a href="https://www.youtube.com/watch?v=Wlmne44M6fQ&amp;t=1s">How to enable SSH on Linux Ubuntu (Easy step by step guide)</a></p><h2 id="Step-2-Step-up-ssh-port-forwarding-to-connect-from-any-network-through-remote-laptops-e-g-mac-windows-to-Ubuntu">Step 2: Step up ssh port forwarding to connect from any network through remote laptops (e.g. mac/windows) to Ubuntu</h2><p><a href="https://www.zhihu.com/zvideo/1621515254902521856">windows 系统如何通过 ssh 远程连接 linux？ - 知乎</a></p><p><a href="https://www.cpolar.com/">cpolar - 安全的内网穿透工具</a></p><blockquote><p>I used the free account that fulfills my personal usage and need. Works perfectly at the moment. There can be other cleaner approaches, but I haven’t find such one. Please send me a message if you found one.</p></blockquote><h2 id="Step-3-passwordless-login-instructions">Step 3: passwordless login instructions</h2><p><a href="https://levelup.gitconnected.com/how-to-connect-without-password-using-ssh-passwordless-9b8963c828e8">How to connect without password using SSH (passwordless)</a></p><div class="note note-secondary">            <p>Thanks for reading, if you have any questions/suggestions, please leave a comment below.</p>          </div>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>template post with syntax</title>
    <link href="/blog/2023/04/21/template-post-with-syntax/"/>
    <url>/blog/2023/04/21/template-post-with-syntax/</url>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p><a href="https://hexo.fluid-dev.com/docs/guide/">Source</a><br>This is a page that showcases all the syntax that are supported by fluid and hexo.</p>          </div><h1>Index image in home page</h1><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs markdown">---<br>title: 文章标题<br>tags: [Hexo, Fluid]<br>index<span class="hljs-emphasis">_img: /img/example.jpg</span><br><span class="hljs-emphasis">date: 2019-10-10 10:00:00</span><br><span class="hljs-emphasis">---</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">以下是文章内容</span><br></code></pre></td></tr></table></figure><h1>Codeblocks</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br></code></pre></td></tr></table></figure><h1>Latex formulas</h1><p>$$<br>E=mc^2<br>$$</p><h1>Mermaid diagrams <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Links: [Source](https://mermaid-js.github.io/mermaid/#/); [Github](https://github.com/mermaid-js/mermaid)">[1]</span></a></sup></h1><h2 id="Gannt-diagram">Gannt diagram</h2><pre><code class="mermaid" >ganttdateFormat YYYY-MM-DDtitle Adding GANTT diagram to mermaidsection A sectionCompleted task :done, des1, 2014-01-06,2014-01-08Active task :active, des2, 2014-01-09, 3dFuture task : des3, after des2, 5dFuture task2 : des4, after des3, 5d</code></pre><h2 id="classDiagram">classDiagram</h2><pre><code class=" mermaid">classDiagramClass01 &lt;|-- AveryLongClass : CoolClass03 *-- Class04Class05 o-- Class06Class07 .. Class08Class09 --&gt; C2 : Where am i?Class09 --* C3Class09 --|&gt; Class07Class07 : equals()Class07 : Object[] elementDataClass01 : size()Class01 : int chimpClass01 : int gorillaClass08 &lt;--&gt; C2: Cool label</code></pre><h2 id="Graph">Graph</h2><pre><code class=" mermaid">graph TD;    A--&gt;B;    A--&gt;C;    B--&gt;D;    C--&gt;D;</code></pre><h2 id="Sequence-diagram">Sequence diagram</h2><pre><code class=" mermaid">sequenceDiagram    participant Alice    participant Bob    Alice-&gt;&gt;John: Hello John, how are you?    loop Healthcheck        John-&gt;&gt;John: Fight against hypochondria    end    Note right of John: Rational thoughts &lt;br/&gt;prevail!    John--&gt;&gt;Alice: Great!    John-&gt;&gt;Bob: How about you?    Bob--&gt;&gt;John: Jolly good!</code></pre><h2 id="Git-graph">Git graph</h2><pre><code class=" mermaid">gitGraph   commit   commit   branch develop   checkout develop   commit   commit   checkout main   merge develop   commit   commit</code></pre><h2 id="Flowchart">Flowchart</h2><pre><code class=" mermaid">flowchart TD    A[Start] --&gt; B&#123;Is it?&#125;    B -- Yes --&gt; C[OK]    C --&gt; D[Rethink]    D --&gt; B    B -- No ----&gt; E[End]</code></pre><h1>Using footnotes</h1><p>This is a sentence<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="This is the footnote content">[2]</span></a></sup></p><p>This is a sentence<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="This is the footnote content">[2]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="This is the footnote content">[3]</span></a></sup></p><p>This is a sentence<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="This is the footnote content">[2]</span></a></sup></p><p>This is a sentence<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="This is the footnote content">[2]</span></a></sup></p><section class="footnotes"><h1>Reference</h1><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Links: <a href="https://mermaid-js.github.io/mermaid/#/">Source</a>; <a href="https://github.com/mermaid-js/mermaid">Github</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>This is the footnote content<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>This is the footnote content<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
